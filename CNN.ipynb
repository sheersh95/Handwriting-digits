{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import Dense  \n",
    "from keras.models import Sequential  \n",
    "import pickle\n",
    "import gzip \n",
    "import numpy as np\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers import Dense\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.metrics import accuracy_score\n",
    "from PIL import Image\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10  \n",
    "image_vector_size = 28*28  \n",
    "image_size = 784 \n",
    "input_shape = (1, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'mnist.pkl.gz'\n",
    "f = gzip.open(filename, 'rb')\n",
    "training_data, validation_data, test_data = pickle.load(f, encoding='latin1')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "USPSMat  = []\n",
    "USPSTar  = []\n",
    "curPath  = 'USPSdata/USPSdata/Numerals'\n",
    "savedImg = []\n",
    "\n",
    "for j in range(0,10):\n",
    "    curFolderPath = curPath + '/' + str(j)\n",
    "    imgs =  os.listdir(curFolderPath)\n",
    "    for img in imgs:\n",
    "        curImg = curFolderPath + '/' + img\n",
    "        if curImg[-3:] == 'png':\n",
    "            img = Image.open(curImg,'r')\n",
    "            img = img.resize((28, 28))\n",
    "            savedImg = img\n",
    "            imgdata = (255-np.array(img.getdata()))/255\n",
    "            USPSMat.append(imgdata)\n",
    "            USPSTar.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = training_data[0]\n",
    "Y_train = training_data[1]\n",
    "X_test = test_data[0]\n",
    "Y_test = test_data[1]\n",
    "X_val = validation_data[0]\n",
    "Y_val = validation_data[1]\n",
    "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1).astype('float32')\n",
    "X_val = X_val.reshape(X_val.shape[0], 28, 28, 1).astype('float32')\n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1).astype('float32')\n",
    "USPSMat = USPSMat.reshape(USPSMat.shape[0], 28, 28, 1).astype('float32') \n",
    "Y_train = keras.utils.to_categorical(Y_train, num_classes)\n",
    "Y_val = keras.utils.to_categorical(Y_val, num_classes)\n",
    "Y_test = keras.utils.to_categorical(Y_test, num_classes) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# normalize inputs from 0-255 to 0-1\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = Y_test.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (5, 5), input_shape=(28, 28, 1), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/lib/python3/dist-packages/keras/backend/tensorflow_backend.py:1259: calling reduce_prod (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /usr/lib/python3/dist-packages/keras/backend/tensorflow_backend.py:2880: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /usr/lib/python3/dist-packages/keras/backend/tensorflow_backend.py:1344: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      " - 27s - loss: 1.3253 - acc: 0.6417 - val_loss: 0.5344 - val_acc: 0.8510\n",
      "Epoch 2/20\n",
      " - 26s - loss: 0.4603 - acc: 0.8663 - val_loss: 0.3691 - val_acc: 0.8948\n",
      "Epoch 3/20\n",
      " - 26s - loss: 0.3702 - acc: 0.8906 - val_loss: 0.3180 - val_acc: 0.9093\n",
      "Epoch 4/20\n",
      " - 27s - loss: 0.3281 - acc: 0.9021 - val_loss: 0.2881 - val_acc: 0.9152\n",
      "Epoch 5/20\n",
      " - 26s - loss: 0.2917 - acc: 0.9126 - val_loss: 0.2498 - val_acc: 0.9248\n",
      "Epoch 6/20\n",
      " - 27s - loss: 0.2619 - acc: 0.9221 - val_loss: 0.2271 - val_acc: 0.9326\n",
      "Epoch 7/20\n",
      " - 26s - loss: 0.2315 - acc: 0.9314 - val_loss: 0.1928 - val_acc: 0.9429\n",
      "Epoch 8/20\n",
      " - 28s - loss: 0.2041 - acc: 0.9386 - val_loss: 0.1706 - val_acc: 0.9484\n",
      "Epoch 9/20\n",
      " - 27s - loss: 0.1812 - acc: 0.9456 - val_loss: 0.1551 - val_acc: 0.9551\n",
      "Epoch 10/20\n",
      " - 28s - loss: 0.1633 - acc: 0.9507 - val_loss: 0.1392 - val_acc: 0.9604\n",
      "Epoch 11/20\n",
      " - 28s - loss: 0.1448 - acc: 0.9573 - val_loss: 0.1256 - val_acc: 0.9638\n",
      "Epoch 12/20\n",
      " - 27s - loss: 0.1305 - acc: 0.9606 - val_loss: 0.1095 - val_acc: 0.9671\n",
      "Epoch 13/20\n",
      " - 26s - loss: 0.1187 - acc: 0.9649 - val_loss: 0.1045 - val_acc: 0.9700\n",
      "Epoch 14/20\n",
      " - 28s - loss: 0.1091 - acc: 0.9676 - val_loss: 0.0953 - val_acc: 0.9716\n",
      "Epoch 15/20\n",
      " - 26s - loss: 0.1009 - acc: 0.9703 - val_loss: 0.0881 - val_acc: 0.9734\n",
      "Epoch 16/20\n",
      " - 26s - loss: 0.0927 - acc: 0.9727 - val_loss: 0.0822 - val_acc: 0.9755\n",
      "Epoch 17/20\n",
      " - 26s - loss: 0.0868 - acc: 0.9743 - val_loss: 0.0756 - val_acc: 0.9763\n",
      "Epoch 18/20\n",
      " - 27s - loss: 0.0822 - acc: 0.9756 - val_loss: 0.0751 - val_acc: 0.9757\n",
      "Epoch 19/20\n",
      " - 30s - loss: 0.0770 - acc: 0.9769 - val_loss: 0.0683 - val_acc: 0.9789\n",
      "Epoch 20/20\n",
      " - 28s - loss: 0.0719 - acc: 0.9787 - val_loss: 0.0652 - val_acc: 0.9784\n",
      "CNN Error: 2.16%\n"
     ]
    }
   ],
   "source": [
    "# build the model\n",
    "model = baseline_model()\n",
    "# Fit the model\n",
    "model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=20, batch_size=200, verbose=2)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print(\"CNN Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98172"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PredictNN = model.predict(X_train)\n",
    "PredNN = []\n",
    "for i in range(0,50000):\n",
    "    PredNN.append(np.argmax(PredictNN[i]))\n",
    "    \n",
    "accuracy_score(training_data[1],PredNN) #Training Accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4893,    3,    1,    0,    3,    4,   13,    1,    8,    6],\n",
       "       [   1, 5632,   19,    2,    6,    0,    0,    6,    9,    3],\n",
       "       [   9,   21, 4851,   15,   10,    0,    2,   27,   25,    8],\n",
       "       [   8,   11,   27, 4940,    0,   22,    1,   23,   46,   23],\n",
       "       [   1,   10,    3,    0, 4805,    0,   12,    2,    5,   21],\n",
       "       [   8,   11,    2,   10,    2, 4402,   21,    4,   26,   20],\n",
       "       [  13,    5,    1,    0,    9,    5, 4902,    1,   15,    0],\n",
       "       [   3,   15,   19,    2,   15,    2,    0, 5074,    6,   39],\n",
       "       [  10,   35,    7,    8,   17,   10,   14,    6, 4717,   18],\n",
       "       [   6,   10,    0,   11,   43,    8,    1,   26,   13, 4870]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(training_data[1],PredNN) #Confusion Matrix for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9578"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PredictNN = model.predict(X_val)\n",
    "PredNN = []\n",
    "for i in range(0,10000):\n",
    "    PredNN.append(np.argmax(PredictNN[i]))\n",
    "    \n",
    "accuracy_score(validation_data[1],PredNN) #Training Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 984,    0,    0,    0,    1,    0,    1,    3,    0,    2],\n",
       "       [   0, 1057,    1,    3,    0,    0,    0,    2,    1,    0],\n",
       "       [   3,   12,  949,    7,    4,    0,    1,   14,    0,    0],\n",
       "       [   2,    1,    2, 1011,    1,    6,    0,    2,    2,    3],\n",
       "       [   0,    7,    0,    0,  970,    0,    0,    1,    0,    5],\n",
       "       [   6,    1,    2,   13,    2,  874,    9,    0,    2,    6],\n",
       "       [   5,    6,    0,    0,    8,    3,  944,    0,    1,    0],\n",
       "       [   1,    2,    1,    1,    3,    0,    0, 1079,    0,    3],\n",
       "       [   7,   51,    9,   32,    5,   11,    6,   19,  862,    7],\n",
       "       [   3,   11,    0,    7,   40,    3,    0,   49,    0,  848]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(validation_data[1],PredNN) #Confusion Matrix for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9565"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PredictNN = model.predict(X_test)\n",
    "PredNN = []\n",
    "for i in range(0,10000):\n",
    "    PredNN.append(np.argmax(PredictNN[i]))\n",
    "    \n",
    "accuracy_score(test_data[1],PredNN) #Training Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 975,    0,    0,    1,    2,    0,    0,    1,    0,    1],\n",
       "       [   0, 1131,    2,    0,    0,    0,    1,    0,    1,    0],\n",
       "       [   7,    6,  990,   11,    4,    0,    0,   12,    1,    1],\n",
       "       [   1,    0,    2,  989,    0,    5,    0,   10,    3,    0],\n",
       "       [   0,    0,    0,    0,  977,    0,    0,    4,    0,    1],\n",
       "       [   3,    1,    0,   10,    1,  868,    5,    1,    1,    2],\n",
       "       [  11,    4,    1,    1,   12,    8,  921,    0,    0,    0],\n",
       "       [   1,    3,    7,    0,    1,    0,    0, 1012,    1,    3],\n",
       "       [  15,   31,   10,   33,   11,    8,   10,   23,  827,    6],\n",
       "       [   5,   11,    1,    7,   41,    3,    0,   66,    0,  875]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(test_data[1],PredNN) #Confusion Matrix for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47297364868243413"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PredictNN = model.predict(USPSMat)\n",
    "PredNN = []\n",
    "for i in range(0,19999):\n",
    "    PredNN.append(np.argmax(PredictNN[i]))\n",
    "    \n",
    "accuracy_score(USPSTar,PredNN) #Training Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 440,    4,   14,   59,  722,   26,   28,  120,   90,  497],\n",
       "       [  70,  351,  217,   22,  713,   18,   12,  480,  104,   13],\n",
       "       [  70,   19, 1476,  104,  144,   47,   35,   46,   48,   10],\n",
       "       [  23,   11,  119, 1471,   23,  230,    0,   27,   90,    6],\n",
       "       [   3,    4,   20,    4, 1417,   13,    3,  237,  265,   34],\n",
       "       [  71,    9,   25,  144,   57, 1512,   18,   23,  109,   32],\n",
       "       [ 316,   42,  270,   38,  178,  130,  855,    5,   70,   96],\n",
       "       [  48,   41,   79,  442,   81,   25,    5, 1060,  191,   28],\n",
       "       [ 185,    7,   74,  645,  145,  248,   29,  143,  454,   70],\n",
       "       [   9,    4,   33,  278,  173,   22,    0,  708,  350,  423]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(USPSTar,PredNN) #Confusion Matrix for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19999, 10)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PredictNN.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
